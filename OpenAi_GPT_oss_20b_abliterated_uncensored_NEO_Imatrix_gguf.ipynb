{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "generative_ai_disabled": true,
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9c220bb2c5004a99bb3543c6ca202353": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9ad335b73b404f559ca2047da3c76717",
              "IPY_MODEL_46b9c487c1a1482dbf91b99337813413",
              "IPY_MODEL_9097a94c260c4d5d82a72cb0d6117bf8"
            ],
            "layout": "IPY_MODEL_6272e6985f0c4441a8f18df7d162e87e"
          }
        },
        "9ad335b73b404f559ca2047da3c76717": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a9f808325e046e2bd19f7ea9a5ae4e8",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f6612a4f28624487a5c10b78600398ac",
            "value": "./OpenAI-20B-NEO-CODE-DI-Uncensored-Q5_1(‚Ä¶):‚Äá100%"
          }
        },
        "46b9c487c1a1482dbf91b99337813413": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd687a4767684c5380b7a7841cf32c6a",
            "max": 15728919168,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5a29efe0a2344eeb9ffe3fba06f84b9c",
            "value": 15728919168
          }
        },
        "9097a94c260c4d5d82a72cb0d6117bf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06e90ce3dff240389682bc32bf6f4b2e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_01b91dfd03ef480d9f19a664ccf0a9cb",
            "value": "‚Äá15.7G/15.7G‚Äá[02:03&lt;00:00,‚Äá242MB/s]"
          }
        },
        "6272e6985f0c4441a8f18df7d162e87e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a9f808325e046e2bd19f7ea9a5ae4e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6612a4f28624487a5c10b78600398ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd687a4767684c5380b7a7841cf32c6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a29efe0a2344eeb9ffe3fba06f84b9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "06e90ce3dff240389682bc32bf6f4b2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01b91dfd03ef480d9f19a664ccf0a9cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/subhasishsahugit/AI_ML/blob/main/OpenAi_GPT_oss_20b_abliterated_uncensored_NEO_Imatrix_gguf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U llama-cpp-python"
      ],
      "metadata": {
        "id": "6EMZ6SSdmKXn",
        "outputId": "cc9a00c1-90f0-47ab-9773-59b3b1e300c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.3.16.tar.gz (50.7 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.7/50.7 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (2.0.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.3)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl size=4503278 sha256=ebd99480d7e831dbb5db4e28e343b17d6cd9dbf58b2eae8e8ca7ee501f45675d\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/82/ab/8784ee3fb99ddb07fd36a679ddbe63122cc07718f6c1eb3be8\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Local Inference on GPU\n",
        "Model page: https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf\n",
        "\n",
        "‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf)\n",
        "\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè"
      ],
      "metadata": {
        "id": "IQ5arxQUmKXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install llama-cpp-python\n",
        "\n",
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama.from_pretrained(\n",
        "    repo_id=\"DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf\",\n",
        "    filename=\"OpenAI-20B-NEO-CODE-DI-Uncensored-Q5_1.gguf\",\n",
        ")"
      ],
      "metadata": {
        "id": "4cszyT3ymKXs",
        "outputId": "422757e5-52ee-4375-bc08-7bc15544be88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9c220bb2c5004a99bb3543c6ca202353",
            "9ad335b73b404f559ca2047da3c76717",
            "46b9c487c1a1482dbf91b99337813413",
            "9097a94c260c4d5d82a72cb0d6117bf8",
            "6272e6985f0c4441a8f18df7d162e87e",
            "7a9f808325e046e2bd19f7ea9a5ae4e8",
            "f6612a4f28624487a5c10b78600398ac",
            "fd687a4767684c5380b7a7841cf32c6a",
            "5a29efe0a2344eeb9ffe3fba06f84b9c",
            "06e90ce3dff240389682bc32bf6f4b2e",
            "01b91dfd03ef480d9f19a664ccf0a9cb"
          ]
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "./OpenAI-20B-NEO-CODE-DI-Uncensored-Q5_1(‚Ä¶):   0%|          | 0.00/15.7G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c220bb2c5004a99bb3543c6ca202353"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 44 key-value pairs and 459 tensors from /root/.cache/huggingface/hub/models--DavidAU--OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/snapshots/92d1bcc0244193a2cd11b9da138dfee9e5203d37/./OpenAI-20B-NEO-CODE-DI-Uncensored-Q5_1.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = gpt-oss\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Huihui Gpt Oss 20b BF16 Abliterated\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = abliterated\n",
            "llama_model_loader: - kv   4:                           general.basename str              = Huihui-gpt-oss\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 20B\n",
            "llama_model_loader: - kv   6:                            general.license str              = apache-2.0\n",
            "llama_model_loader: - kv   7:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv   8:                  general.base_model.0.name str              = Gpt Oss 20b BF16\n",
            "llama_model_loader: - kv   9:          general.base_model.0.organization str              = Unsloth\n",
            "llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/unsloth/gpt-os...\n",
            "llama_model_loader: - kv  11:                               general.tags arr[str,5]       = [\"vllm\", \"unsloth\", \"abliterated\", \"u...\n",
            "llama_model_loader: - kv  12:                        gpt-oss.block_count u32              = 24\n",
            "llama_model_loader: - kv  13:                     gpt-oss.context_length u32              = 131072\n",
            "llama_model_loader: - kv  14:                   gpt-oss.embedding_length u32              = 2880\n",
            "llama_model_loader: - kv  15:                gpt-oss.feed_forward_length u32              = 2880\n",
            "llama_model_loader: - kv  16:               gpt-oss.attention.head_count u32              = 64\n",
            "llama_model_loader: - kv  17:            gpt-oss.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  18:                     gpt-oss.rope.freq_base f32              = 150000.000000\n",
            "llama_model_loader: - kv  19:   gpt-oss.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  20:                       gpt-oss.expert_count u32              = 32\n",
            "llama_model_loader: - kv  21:                  gpt-oss.expert_used_count u32              = 4\n",
            "llama_model_loader: - kv  22:               gpt-oss.attention.key_length u32              = 64\n",
            "llama_model_loader: - kv  23:             gpt-oss.attention.value_length u32              = 64\n",
            "llama_model_loader: - kv  24:           gpt-oss.attention.sliding_window u32              = 128\n",
            "llama_model_loader: - kv  25:         gpt-oss.expert_feed_forward_length u32              = 2880\n",
            "llama_model_loader: - kv  26:                  gpt-oss.rope.scaling.type str              = yarn\n",
            "llama_model_loader: - kv  27:                gpt-oss.rope.scaling.factor f32              = 32.000000\n",
            "llama_model_loader: - kv  28: gpt-oss.rope.scaling.original_context_length u32              = 4096\n",
            "llama_model_loader: - kv  29:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  30:                         tokenizer.ggml.pre str              = gpt-4o\n",
            "llama_model_loader: - kv  31:                      tokenizer.ggml.tokens arr[str,201088]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  32:                  tokenizer.ggml.token_type arr[i32,201088]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  33:                      tokenizer.ggml.merges arr[str,446189]  = [\"ƒ† ƒ†\", \"ƒ† ƒ†ƒ†ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"...\n",
            "llama_model_loader: - kv  34:                tokenizer.ggml.bos_token_id u32              = 199998\n",
            "llama_model_loader: - kv  35:                tokenizer.ggml.eos_token_id u32              = 200002\n",
            "llama_model_loader: - kv  36:            tokenizer.ggml.padding_token_id u32              = 199999\n",
            "llama_model_loader: - kv  37:                    tokenizer.chat_template str              = {# Copyright 2025-present Unsloth. Ap...\n",
            "llama_model_loader: - kv  38:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  39:                          general.file_type u32              = 9\n",
            "llama_model_loader: - kv  40:                      quantize.imatrix.file str              = E:/_imx/OpenAi-GPT-oss-20b-abliterate...\n",
            "llama_model_loader: - kv  41:                   quantize.imatrix.dataset str              = f:/llamacpp/_raw_imatrix/neo1-v2.txt\n",
            "llama_model_loader: - kv  42:             quantize.imatrix.entries_count u32              = 193\n",
            "llama_model_loader: - kv  43:              quantize.imatrix.chunks_count u32              = 324\n",
            "llama_model_loader: - type  f32:  289 tensors\n",
            "llama_model_loader: - type q5_1:  170 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q5_1\n",
            "print_info: file size   = 14.64 GiB (6.01 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 2\n",
            "load: control token: 200017 '<|reserved_200017|>' is not marked as EOG\n",
            "load: control token: 200014 '<|reserved_200014|>' is not marked as EOG\n",
            "load: control token: 200011 '<|reserved_200011|>' is not marked as EOG\n",
            "load: control token: 200009 '<|reserved_200009|>' is not marked as EOG\n",
            "load: control token: 200008 '<|message|>' is not marked as EOG\n",
            "load: control token: 200006 '<|start|>' is not marked as EOG\n",
            "load: control token: 200004 '<|reserved_200004|>' is not marked as EOG\n",
            "load: control token: 200003 '<|constrain|>' is not marked as EOG\n",
            "load: control token: 200000 '<|reserved_200000|>' is not marked as EOG\n",
            "load: control token: 200005 '<|channel|>' is not marked as EOG\n",
            "load: control token: 200010 '<|reserved_200010|>' is not marked as EOG\n",
            "load: control token: 200016 '<|reserved_200016|>' is not marked as EOG\n",
            "load: control token: 200013 '<|reserved_200013|>' is not marked as EOG\n",
            "load: control token: 199998 '<|startoftext|>' is not marked as EOG\n",
            "load: control token: 200018 '<|endofprompt|>' is not marked as EOG\n",
            "load: control token: 200001 '<|reserved_200001|>' is not marked as EOG\n",
            "load: control token: 200015 '<|reserved_200015|>' is not marked as EOG\n",
            "load: printing all EOG tokens:\n",
            "load:   - 199999 ('<|endoftext|>')\n",
            "load:   - 200002 ('<|return|>')\n",
            "load:   - 200007 ('<|end|>')\n",
            "load:   - 200012 ('<|call|>')\n",
            "load: special_eog_ids contains both '<|return|>' and '<|call|>' tokens, removing '<|end|>' token from EOG list\n",
            "load: special tokens cache size = 21\n",
            "load: token to piece cache size = 1.3332 MB\n",
            "print_info: arch             = gpt-oss\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 131072\n",
            "print_info: n_embd           = 2880\n",
            "print_info: n_layer          = 24\n",
            "print_info: n_head           = 64\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 64\n",
            "print_info: n_swa            = 128\n",
            "print_info: is_swa_any       = 1\n",
            "print_info: n_embd_head_k    = 64\n",
            "print_info: n_embd_head_v    = 64\n",
            "print_info: n_gqa            = 8\n",
            "print_info: n_embd_k_gqa     = 512\n",
            "print_info: n_embd_v_gqa     = 512\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 2880\n",
            "print_info: n_expert         = 32\n",
            "print_info: n_expert_used    = 4\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = yarn\n",
            "print_info: freq_base_train  = 150000.0\n",
            "print_info: freq_scale_train = 0.03125\n",
            "print_info: n_ctx_orig_yarn  = 4096\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = ?B\n",
            "print_info: model params     = 20.91 B\n",
            "print_info: general.name     = Huihui Gpt Oss 20b BF16 Abliterated\n",
            "print_info: n_ff_exp         = 2880\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 201088\n",
            "print_info: n_merges         = 446189\n",
            "print_info: BOS token        = 199998 '<|startoftext|>'\n",
            "print_info: EOS token        = 200002 '<|return|>'\n",
            "print_info: EOT token        = 199999 '<|endoftext|>'\n",
            "print_info: PAD token        = 199999 '<|endoftext|>'\n",
            "print_info: LF token         = 198 'ƒä'\n",
            "print_info: EOG token        = 199999 '<|endoftext|>'\n",
            "print_info: EOG token        = 200002 '<|return|>'\n",
            "print_info: EOG token        = 200012 '<|call|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   2 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   4 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   6 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   8 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  10 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  12 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  14 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  16 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  18 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  20 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  22 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
            "load_tensors: tensor 'token_embd.weight' (q5_1) (and 458 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
            "load_tensors:   CPU_Mapped model buffer size = 14987.86 MiB\n",
            ".............................................................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 512\n",
            "llama_context: n_ctx_per_seq = 512\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: kv_unified    = false\n",
            "llama_context: freq_base     = 150000.0\n",
            "llama_context: freq_scale    = 0.03125\n",
            "llama_context: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "set_abort_callback: call\n",
            "llama_context:        CPU  output buffer size =     0.77 MiB\n",
            "create_memory: n_ctx = 512 (padded)\n",
            "llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n",
            "llama_kv_cache_unified_iswa: creating non-SWA KV cache, size = 512 cells\n",
            "llama_kv_cache_unified: layer   0: skipped\n",
            "llama_kv_cache_unified: layer   1: dev = CPU\n",
            "llama_kv_cache_unified: layer   2: skipped\n",
            "llama_kv_cache_unified: layer   3: dev = CPU\n",
            "llama_kv_cache_unified: layer   4: skipped\n",
            "llama_kv_cache_unified: layer   5: dev = CPU\n",
            "llama_kv_cache_unified: layer   6: skipped\n",
            "llama_kv_cache_unified: layer   7: dev = CPU\n",
            "llama_kv_cache_unified: layer   8: skipped\n",
            "llama_kv_cache_unified: layer   9: dev = CPU\n",
            "llama_kv_cache_unified: layer  10: skipped\n",
            "llama_kv_cache_unified: layer  11: dev = CPU\n",
            "llama_kv_cache_unified: layer  12: skipped\n",
            "llama_kv_cache_unified: layer  13: dev = CPU\n",
            "llama_kv_cache_unified: layer  14: skipped\n",
            "llama_kv_cache_unified: layer  15: dev = CPU\n",
            "llama_kv_cache_unified: layer  16: skipped\n",
            "llama_kv_cache_unified: layer  17: dev = CPU\n",
            "llama_kv_cache_unified: layer  18: skipped\n",
            "llama_kv_cache_unified: layer  19: dev = CPU\n",
            "llama_kv_cache_unified: layer  20: skipped\n",
            "llama_kv_cache_unified: layer  21: dev = CPU\n",
            "llama_kv_cache_unified: layer  22: skipped\n",
            "llama_kv_cache_unified: layer  23: dev = CPU\n",
            "llama_kv_cache_unified:        CPU KV buffer size =    12.00 MiB\n",
            "llama_kv_cache_unified: size =   12.00 MiB (   512 cells,  12 layers,  1/1 seqs), K (f16):    6.00 MiB, V (f16):    6.00 MiB\n",
            "llama_kv_cache_unified_iswa: creating     SWA KV cache, size = 512 cells\n",
            "llama_kv_cache_unified: layer   0: dev = CPU\n",
            "llama_kv_cache_unified: layer   1: skipped\n",
            "llama_kv_cache_unified: layer   2: dev = CPU\n",
            "llama_kv_cache_unified: layer   3: skipped\n",
            "llama_kv_cache_unified: layer   4: dev = CPU\n",
            "llama_kv_cache_unified: layer   5: skipped\n",
            "llama_kv_cache_unified: layer   6: dev = CPU\n",
            "llama_kv_cache_unified: layer   7: skipped\n",
            "llama_kv_cache_unified: layer   8: dev = CPU\n",
            "llama_kv_cache_unified: layer   9: skipped\n",
            "llama_kv_cache_unified: layer  10: dev = CPU\n",
            "llama_kv_cache_unified: layer  11: skipped\n",
            "llama_kv_cache_unified: layer  12: dev = CPU\n",
            "llama_kv_cache_unified: layer  13: skipped\n",
            "llama_kv_cache_unified: layer  14: dev = CPU\n",
            "llama_kv_cache_unified: layer  15: skipped\n",
            "llama_kv_cache_unified: layer  16: dev = CPU\n",
            "llama_kv_cache_unified: layer  17: skipped\n",
            "llama_kv_cache_unified: layer  18: dev = CPU\n",
            "llama_kv_cache_unified: layer  19: skipped\n",
            "llama_kv_cache_unified: layer  20: dev = CPU\n",
            "llama_kv_cache_unified: layer  21: skipped\n",
            "llama_kv_cache_unified: layer  22: dev = CPU\n",
            "llama_kv_cache_unified: layer  23: skipped\n",
            "llama_kv_cache_unified:        CPU KV buffer size =    12.00 MiB\n",
            "llama_kv_cache_unified: size =   12.00 MiB (   512 cells,  12 layers,  1/1 seqs), K (f16):    6.00 MiB, V (f16):    6.00 MiB\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 1\n",
            "llama_context: max_nodes = 3672\n",
            "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "llama_context:        CPU compute buffer size =   398.38 MiB\n",
            "llama_context: graph nodes  = 1446\n",
            "llama_context: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "Model metadata: {'quantize.imatrix.chunks_count': '324', 'quantize.imatrix.file': 'E:/_imx/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-CODE-OT.gguf', 'general.quantization_version': '2', 'tokenizer.chat_template': '{# Copyright 2025-present Unsloth. Apache 2.0 License. Unsloth chat template fixes. Edited from ggml-org & OpenAI #}\\n{#-\\n  In addition to the normal inputs of `messages` and `tools`, this template also accepts the\\n  following kwargs:\\n  - \"builtin_tools\": A list, can contain \"browser\" and/or \"python\".\\n  - \"model_identity\": A string that optionally describes the model identity.\\n  - \"reasoning_effort\": A string that describes the reasoning effort, defaults to \"medium\".\\n #}\\n\\n{#- Tool Definition Rendering ============================================== #}\\n{%- macro render_typescript_type(param_spec, required_params, is_nullable=false) -%}\\n    {%- if param_spec.type == \"array\" -%}\\n        {%- if param_spec[\\'items\\'] -%}\\n            {%- if param_spec[\\'items\\'][\\'type\\'] == \"string\" -%}\\n                {{- \"string[]\" }}\\n            {%- elif param_spec[\\'items\\'][\\'type\\'] == \"number\" -%}\\n                {{- \"number[]\" }}\\n            {%- elif param_spec[\\'items\\'][\\'type\\'] == \"integer\" -%}\\n                {{- \"number[]\" }}\\n            {%- elif param_spec[\\'items\\'][\\'type\\'] == \"boolean\" -%}\\n                {{- \"boolean[]\" }}\\n            {%- else -%}\\n                {%- set inner_type = render_typescript_type(param_spec[\\'items\\'], required_params) -%}\\n                {%- if inner_type == \"object | object\" or inner_type|length > 50 -%}\\n                    {{- \"any[]\" }}\\n                {%- else -%}\\n                    {{- inner_type + \"[]\" }}\\n                {%- endif -%}\\n            {%- endif -%}\\n            {%- if param_spec.nullable -%}\\n                {{- \" | null\" }}\\n            {%- endif -%}\\n        {%- else -%}\\n            {{- \"any[]\" }}\\n            {%- if param_spec.nullable -%}\\n                {{- \" | null\" }}\\n            {%- endif -%}\\n        {%- endif -%}\\n    {%- elif param_spec.type is defined and param_spec.type is iterable and param_spec.type is not string and param_spec.type is not mapping and param_spec.type[0] is defined -%}\\n        {#- Handle array of types like [\"object\", \"object\"] from Union[dict, list] #}\\n        {%- if param_spec.type | length > 1 -%}\\n            {{- param_spec.type | join(\" | \") }}\\n        {%- else -%}\\n            {{- param_spec.type[0] }}\\n        {%- endif -%}\\n    {%- elif param_spec.oneOf -%}\\n        {#- Handle oneOf schemas - check for complex unions and fallback to any #}\\n        {%- set has_object_variants = false -%}\\n        {%- for variant in param_spec.oneOf -%}\\n            {%- if variant.type == \"object\" -%}\\n                {%- set has_object_variants = true -%}\\n            {%- endif -%}\\n        {%- endfor -%}\\n        {%- if has_object_variants and param_spec.oneOf|length > 1 -%}\\n            {{- \"any\" }}\\n        {%- else -%}\\n            {%- for variant in param_spec.oneOf -%}\\n                {{- render_typescript_type(variant, required_params) -}}\\n                {%- if variant.description %}\\n                    {{- \"// \" + variant.description }}\\n                {%- endif -%}\\n                {%- if variant.default is defined %}\\n                    {{ \"// default: \" + variant.default|tojson }}\\n                {%- endif -%}\\n                {%- if not loop.last %}\\n                    {{- \" | \" }}\\n                {% endif -%}\\n            {%- endfor -%}\\n        {%- endif -%}\\n    {%- elif param_spec.type == \"string\" -%}\\n        {%- if param_spec.enum -%}\\n            {{- \\'\"\\' + param_spec.enum|join(\\'\" | \"\\') + \\'\"\\' -}}\\n        {%- else -%}\\n            {{- \"string\" }}\\n            {%- if param_spec.nullable %}\\n                {{- \" | null\" }}\\n            {%- endif -%}\\n        {%- endif -%}\\n    {%- elif param_spec.type == \"number\" -%}\\n        {{- \"number\" }}\\n    {%- elif param_spec.type == \"integer\" -%}\\n        {{- \"number\" }}\\n    {%- elif param_spec.type == \"boolean\" -%}\\n        {{- \"boolean\" }}\\n\\n    {%- elif param_spec.type == \"object\" -%}\\n        {%- if param_spec.properties -%}\\n            {{- \"{\\\\n\" }}\\n            {%- for prop_name, prop_spec in param_spec.properties.items() -%}\\n                {{- prop_name -}}\\n                {%- if prop_name not in (param_spec.required or []) -%}\\n                    {{- \"?\" }}\\n                {%- endif -%}\\n                {{- \": \" }}\\n                {{ render_typescript_type(prop_spec, param_spec.required or []) }}\\n                {%- if not loop.last -%}\\n                    {{-\", \" }}\\n                {%- endif -%}\\n            {%- endfor -%}\\n            {{- \"}\" }}\\n        {%- else -%}\\n            {{- \"object\" }}\\n        {%- endif -%}\\n    {%- else -%}\\n        {{- \"any\" }}\\n    {%- endif -%}\\n{%- endmacro -%}\\n\\n{%- macro render_tool_namespace(namespace_name, tools) -%}\\n    {{- \"## \" + namespace_name + \"\\\\n\\\\n\" }}\\n    {{- \"namespace \" + namespace_name + \" {\\\\n\\\\n\" }}\\n    {%- for tool in tools %}\\n        {%- set tool = tool.function %}\\n        {{- \"// \" + tool.description + \"\\\\n\" }}\\n        {{- \"type \"+ tool.name + \" = \" }}\\n        {%- if tool.parameters and tool.parameters.properties -%}\\n            {{- \"(_: \" }}\\n            {{- \"{\\\\n\" }}\\n            {%- for param_name, param_spec in tool.parameters.properties.items() %}\\n                {{- \"// \" + param_spec.description + \"\\\\n\" }}\\n                {{- param_name }}\\n                {%- if param_name not in (tool.parameters.required or []) -%}\\n                    {{- \"?\" }}\\n                {%- endif -%}\\n                {{- \": \" }}\\n                {{- render_typescript_type(param_spec, tool.parameters.required or []) }}\\n                {%- if param_spec.default is defined -%}\\n                    {%- if param_spec.enum %}\\n                        {{- \", // default: \" + param_spec.default }}\\n                    {%- elif param_spec.oneOf %}\\n                        {{- \"// default: \" + param_spec.default }}\\n                    {%- else %}\\n                        {{- \", // default: \" + param_spec.default|tojson }}\\n                    {%- endif -%}\\n                {%- endif -%}\\n                {%- if not loop.last %}\\n                    {{- \",\\\\n\" }}\\n                {%- else %}\\n                    {{- \"\\\\n\" }}\\n                {%- endif -%}\\n            {%- endfor %}\\n            {{- \"}) => any;\\\\n\\\\n\" }}\\n        {%- else -%}\\n            {{- \"() => any;\\\\n\\\\n\" }}\\n        {%- endif -%}\\n    {%- endfor %}\\n    {{- \"} // namespace \" + namespace_name }}\\n{%- endmacro -%}\\n\\n{%- macro render_builtin_tools(browser_tool, python_tool) -%}\\n    {%- if browser_tool %}\\n        {{- \"## browser\\\\n\\\\n\" }}\\n        {{- \"// Tool for browsing.\\\\n\" }}\\n        {{- \"// The `cursor` appears in brackets before each browsing display: `[{cursor}]`.\\\\n\" }}\\n        {{- \"// Cite information from the tool using the following format:\\\\n\" }}\\n        {{- \"// `„Äê{cursor}‚Ä†L{line_start}(-L{line_end})?„Äë`, for example: `„Äê6‚Ä†L9-L11„Äë` or `„Äê8‚Ä†L3„Äë`.\\\\n\" }}\\n        {{- \"// Do not quote more than 10 words directly from the tool output.\\\\n\" }}\\n        {{- \"// sources=web (default: web)\\\\n\" }}\\n        {{- \"namespace browser {\\\\n\\\\n\" }}\\n        {{- \"// Searches for information related to `query` and displays `topn` results.\\\\n\" }}\\n        {{- \"type search = (_: {\\\\n\" }}\\n        {{- \"query: string,\\\\n\" }}\\n        {{- \"topn?: number, // default: 10\\\\n\" }}\\n        {{- \"source?: string,\\\\n\" }}\\n        {{- \"}) => any;\\\\n\\\\n\" }}\\n        {{- \"// Opens the link `id` from the page indicated by `cursor` starting at line number `loc`, showing `num_lines` lines.\\\\n\" }}\\n        {{- \"// Valid link ids are displayed with the formatting: `„Äê{id}‚Ä†.*„Äë`.\\\\n\" }}\\n        {{- \"// If `cursor` is not provided, the most recent page is implied.\\\\n\" }}\\n        {{- \"// If `id` is a string, it is treated as a fully qualified URL associated with `source`.\\\\n\" }}\\n        {{- \"// If `loc` is not provided, the viewport will be positioned at the beginning of the document or centered on the most relevant passage, if available.\\\\n\" }}\\n        {{- \"// Use this function without `id` to scroll to a new location of an opened page.\\\\n\" }}\\n        {{- \"type open = (_: {\\\\n\" }}\\n        {{- \"id?: number | string, // default: -1\\\\n\" }}\\n        {{- \"cursor?: number, // default: -1\\\\n\" }}\\n        {{- \"loc?: number, // default: -1\\\\n\" }}\\n        {{- \"num_lines?: number, // default: -1\\\\n\" }}\\n        {{- \"view_source?: boolean, // default: false\\\\n\" }}\\n        {{- \"source?: string,\\\\n\" }}\\n        {{- \"}) => any;\\\\n\\\\n\" }}\\n        {{- \"// Finds exact matches of `pattern` in the current page, or the page given by `cursor`.\\\\n\" }}\\n        {{- \"type find = (_: {\\\\n\" }}\\n        {{- \"pattern: string,\\\\n\" }}\\n        {{- \"cursor?: number, // default: -1\\\\n\" }}\\n        {{- \"}) => any;\\\\n\\\\n\" }}\\n        {{- \"} // namespace browser\\\\n\\\\n\" }}\\n    {%- endif -%}\\n\\n    {%- if python_tool %}\\n        {{- \"## python\\\\n\\\\n\" }}\\n        {{- \"Use this tool to execute Python code in your chain of thought. The code will not be shown to the user. This tool should be used for internal reasoning, but not for code that is intended to be visible to the user (e.g. when creating plots, tables, or files).\\\\n\\\\n\" }}\\n        {{- \"When you send a message containing Python code to python, it will be executed in a stateful Jupyter notebook environment. python will respond with the output of the execution or time out after 120.0 seconds. The drive at \\'/mnt/data\\' can be used to save and persist user files. Internet access for this session is UNKNOWN. Depends on the cluster.\\\\n\\\\n\" }}\\n    {%- endif -%}\\n{%- endmacro -%}\\n\\n{#- System Message Construction ============================================ #}\\n{%- macro build_system_message() -%}\\n    {%- if model_identity is not defined %}\\n        {{- \"You are ChatGPT, a large language model trained by OpenAI.\\\\n\" -}}\\n    {%- else %}\\n        {{- model_identity }}\\n    {%- endif %}\\n    {{- \"Knowledge cutoff: 2024-06\\\\n\" }}\\n    {{- \"Current date: \" + strftime_now(\"%Y-%m-%d\") + \"\\\\n\\\\n\" }}\\n    {%- if reasoning_effort is not defined %}\\n        {%- set reasoning_effort = \"medium\" %}\\n    {%- endif %}\\n    {{- \"Reasoning: \" + reasoning_effort + \"\\\\n\\\\n\" }}\\n    {%- if builtin_tools is defined %}\\n        {{- \"# Tools\\\\n\\\\n\" }}\\n        {%- set available_builtin_tools = namespace(browser=false, python=false) %}\\n        {%- for tool in builtin_tools %}\\n            {%- if tool == \"browser\" %}\\n                {%- set available_builtin_tools.browser = true %}\\n            {%- elif tool == \"python\" %}\\n                {%- set available_builtin_tools.python = true %}\\n            {%- endif %}\\n        {%- endfor %}\\n        {{- render_builtin_tools(available_builtin_tools.browser, available_builtin_tools.python) }}\\n    {%- endif -%}\\n    {{- \"# Valid channels: analysis, commentary, final. Channel must be included for every message.\" }}\\n    {%- if tools is defined -%}\\n        {{- \"\\\\nCalls to these tools must go to the commentary channel: \\'functions\\'.\" }}\\n    {%- endif -%}\\n{%- endmacro -%}\\n\\n{#- Main Template Logic ================================================= #}\\n{#- Set defaults #}\\n\\n{#- Render system message #}\\n{{- \"<|start|>system<|message|>\" }}\\n{{- build_system_message() }}\\n{{- \"<|end|>\" }}\\n\\n{#- Extract developer message #}\\n{%- if messages[0].role == \"developer\" or messages[0].role == \"system\" %}\\n    {%- set developer_message = messages[0].content %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set developer_message = \"\" %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n\\n{#- Render developer message #}\\n{%- if developer_message or tools %}\\n    {{- \"<|start|>developer<|message|>\" }}\\n    {%- if developer_message %}\\n        {{- \"# Instructions\\\\n\\\\n\" }}\\n        {{- developer_message }}\\n    {%- endif %}\\n    {%- if tools -%}\\n        {{- \"\\\\n\\\\n\" }}\\n        {{- \"# Tools\\\\n\\\\n\" }}\\n        {{- render_tool_namespace(\"functions\", tools) }}\\n    {%- endif -%}\\n    {{- \"<|end|>\" }}\\n{%- endif %}\\n\\n{#- Render messages #}\\n{%- set last_tool_call = namespace(name=none) %}\\n{%- for message in loop_messages -%}\\n    {#- At this point only assistant/user/tool messages should remain #}\\n    {%- if message.role == \\'assistant\\' -%}\\n        {%- if \"tool_calls\" in message %}\\n            {#- We assume max 1 tool call per message, and so we infer the tool call name #}\\n            {#- in \"tool\" messages from the most recent assistant tool call name #}\\n            {%- set tool_call = message.tool_calls[0] %}\\n            {%- if tool_call.function %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {%- if message.content %}\\n                {{- \"<|start|>assistant<|channel|>analysis<|message|>\" + message.content + \"<|end|>\" }}\\n            {%- endif %}\\n            {{- \"<|start|>assistant to=\" }}\\n            {{- \"functions.\" + tool_call.name + \"<|channel|>commentary json<|message|>\" }}\\n            {{- tool_call.arguments|tojson }}\\n            {{- \"<|call|>\" }}\\n            {%- set last_tool_call.name = tool_call.name %}\\n        {%- elif \"thinking\" in message and loop.last and not add_generation_prompt %}\\n            {#- Only render the CoT if the final turn is an assistant turn and add_generation_prompt is false #}\\n            {#- This is a situation that should only occur in training, never in inference. #}\\n            {{- \"<|start|>assistant<|channel|>analysis<|message|>\" + message.thinking + \"<|end|>\" }}\\n            {#- <|return|> indicates the end of generation, but <|end|> does not #}\\n            {#- <|return|> should never be an input to the model, but we include it as the final token #}\\n            {#- when training, so the model learns to emit it. #}\\n            {{- \"<|start|>assistant<|channel|>final<|message|>\" + message.content + \"<|return|>\" }}\\n            {%- set last_tool_call.name = none %}\\n        {%- elif \"thinking\" in message %}\\n            {#- CoT is dropped during all previous turns, so we never render it for inference #}\\n            {{- \"<|start|>assistant<|channel|>final<|message|>\" + message.content + \"<|end|>\" }}\\n            {%- set last_tool_call.name = none %}\\n        {%- elif loop.last and not add_generation_prompt %}\\n            {#- <|return|> indicates the end of generation, but <|end|> does not #}\\n            {#- <|return|> should never be an input to the model, but we include it as the final token #}\\n            {#- when training, so the model learns to emit it. #}\\n            {{- \"<|start|>assistant<|message|>\" + message.content + \"<|return|>\" }}\\n        {%- else %}\\n            {{- \"<|start|>assistant<|message|>\" + message.content + \"<|end|>\" }}\\n            {%- set last_tool_call.name = none %}\\n        {%- endif %}\\n    {%- elif message.role == \\'tool\\' -%}\\n        {%- if last_tool_call.name is none %}\\n            {{- raise_exception(\"Message has tool role, but there was no previous assistant message with a tool call!\") }}\\n        {%- endif %}\\n        {{- \"<|start|>functions.\" + last_tool_call.name }}\\n        {{- \" to=assistant<|channel|>commentary<|message|>\" + message.content|tojson + \"<|end|>\" }}\\n    {%- else -%}\\n        {{- \"<|start|>user<|message|>\" + message.content + \"<|end|>\" }}\\n    {%- endif -%}\\n{%- endfor -%}\\n\\n{#- Generation prompt #}\\n{%- if add_generation_prompt -%}\\n<|start|>assistant\\n{%- endif -%}\\n{# Copyright 2025-present Unsloth. Apache 2.0 License. Unsloth chat template fixes. Edited from ggml-org & OpenAI #}', 'tokenizer.ggml.eos_token_id': '200002', 'tokenizer.ggml.pre': 'gpt-4o', 'tokenizer.ggml.padding_token_id': '199999', 'gpt-oss.context_length': '131072', 'general.base_model.0.repo_url': 'https://huggingface.co/unsloth/gpt-oss-20b-BF16', 'general.license': 'apache-2.0', 'general.size_label': '20B', 'general.type': 'model', 'general.file_type': '9', 'general.finetune': 'abliterated', 'gpt-oss.rope.scaling.factor': '32.000000', 'quantize.imatrix.dataset': 'f:/llamacpp/_raw_imatrix/neo1-v2.txt', 'general.base_model.0.name': 'Gpt Oss 20b BF16', 'gpt-oss.embedding_length': '2880', 'quantize.imatrix.entries_count': '193', 'gpt-oss.block_count': '24', 'gpt-oss.attention.sliding_window': '128', 'general.base_model.count': '1', 'general.base_model.0.organization': 'Unsloth', 'general.architecture': 'gpt-oss', 'gpt-oss.rope.freq_base': '150000.000000', 'gpt-oss.feed_forward_length': '2880', 'gpt-oss.attention.head_count': '64', 'gpt-oss.rope.scaling.original_context_length': '4096', 'gpt-oss.attention.head_count_kv': '8', 'gpt-oss.attention.layer_norm_rms_epsilon': '0.000010', 'gpt-oss.expert_count': '32', 'general.basename': 'Huihui-gpt-oss', 'gpt-oss.attention.key_length': '64', 'gpt-oss.expert_used_count': '4', 'tokenizer.ggml.bos_token_id': '199998', 'general.name': 'Huihui Gpt Oss 20b BF16 Abliterated', 'gpt-oss.attention.value_length': '64', 'gpt-oss.expert_feed_forward_length': '2880', 'gpt-oss.rope.scaling.type': 'yarn', 'tokenizer.ggml.model': 'gpt2'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {# Copyright 2025-present Unsloth. Apache 2.0 License. Unsloth chat template fixes. Edited from ggml-org & OpenAI #}\n",
            "{#-\n",
            "  In addition to the normal inputs of `messages` and `tools`, this template also accepts the\n",
            "  following kwargs:\n",
            "  - \"builtin_tools\": A list, can contain \"browser\" and/or \"python\".\n",
            "  - \"model_identity\": A string that optionally describes the model identity.\n",
            "  - \"reasoning_effort\": A string that describes the reasoning effort, defaults to \"medium\".\n",
            " #}\n",
            "\n",
            "{#- Tool Definition Rendering ============================================== #}\n",
            "{%- macro render_typescript_type(param_spec, required_params, is_nullable=false) -%}\n",
            "    {%- if param_spec.type == \"array\" -%}\n",
            "        {%- if param_spec['items'] -%}\n",
            "            {%- if param_spec['items']['type'] == \"string\" -%}\n",
            "                {{- \"string[]\" }}\n",
            "            {%- elif param_spec['items']['type'] == \"number\" -%}\n",
            "                {{- \"number[]\" }}\n",
            "            {%- elif param_spec['items']['type'] == \"integer\" -%}\n",
            "                {{- \"number[]\" }}\n",
            "            {%- elif param_spec['items']['type'] == \"boolean\" -%}\n",
            "                {{- \"boolean[]\" }}\n",
            "            {%- else -%}\n",
            "                {%- set inner_type = render_typescript_type(param_spec['items'], required_params) -%}\n",
            "                {%- if inner_type == \"object | object\" or inner_type|length > 50 -%}\n",
            "                    {{- \"any[]\" }}\n",
            "                {%- else -%}\n",
            "                    {{- inner_type + \"[]\" }}\n",
            "                {%- endif -%}\n",
            "            {%- endif -%}\n",
            "            {%- if param_spec.nullable -%}\n",
            "                {{- \" | null\" }}\n",
            "            {%- endif -%}\n",
            "        {%- else -%}\n",
            "            {{- \"any[]\" }}\n",
            "            {%- if param_spec.nullable -%}\n",
            "                {{- \" | null\" }}\n",
            "            {%- endif -%}\n",
            "        {%- endif -%}\n",
            "    {%- elif param_spec.type is defined and param_spec.type is iterable and param_spec.type is not string and param_spec.type is not mapping and param_spec.type[0] is defined -%}\n",
            "        {#- Handle array of types like [\"object\", \"object\"] from Union[dict, list] #}\n",
            "        {%- if param_spec.type | length > 1 -%}\n",
            "            {{- param_spec.type | join(\" | \") }}\n",
            "        {%- else -%}\n",
            "            {{- param_spec.type[0] }}\n",
            "        {%- endif -%}\n",
            "    {%- elif param_spec.oneOf -%}\n",
            "        {#- Handle oneOf schemas - check for complex unions and fallback to any #}\n",
            "        {%- set has_object_variants = false -%}\n",
            "        {%- for variant in param_spec.oneOf -%}\n",
            "            {%- if variant.type == \"object\" -%}\n",
            "                {%- set has_object_variants = true -%}\n",
            "            {%- endif -%}\n",
            "        {%- endfor -%}\n",
            "        {%- if has_object_variants and param_spec.oneOf|length > 1 -%}\n",
            "            {{- \"any\" }}\n",
            "        {%- else -%}\n",
            "            {%- for variant in param_spec.oneOf -%}\n",
            "                {{- render_typescript_type(variant, required_params) -}}\n",
            "                {%- if variant.description %}\n",
            "                    {{- \"// \" + variant.description }}\n",
            "                {%- endif -%}\n",
            "                {%- if variant.default is defined %}\n",
            "                    {{ \"// default: \" + variant.default|tojson }}\n",
            "                {%- endif -%}\n",
            "                {%- if not loop.last %}\n",
            "                    {{- \" | \" }}\n",
            "                {% endif -%}\n",
            "            {%- endfor -%}\n",
            "        {%- endif -%}\n",
            "    {%- elif param_spec.type == \"string\" -%}\n",
            "        {%- if param_spec.enum -%}\n",
            "            {{- '\"' + param_spec.enum|join('\" | \"') + '\"' -}}\n",
            "        {%- else -%}\n",
            "            {{- \"string\" }}\n",
            "            {%- if param_spec.nullable %}\n",
            "                {{- \" | null\" }}\n",
            "            {%- endif -%}\n",
            "        {%- endif -%}\n",
            "    {%- elif param_spec.type == \"number\" -%}\n",
            "        {{- \"number\" }}\n",
            "    {%- elif param_spec.type == \"integer\" -%}\n",
            "        {{- \"number\" }}\n",
            "    {%- elif param_spec.type == \"boolean\" -%}\n",
            "        {{- \"boolean\" }}\n",
            "\n",
            "    {%- elif param_spec.type == \"object\" -%}\n",
            "        {%- if param_spec.properties -%}\n",
            "            {{- \"{\\n\" }}\n",
            "            {%- for prop_name, prop_spec in param_spec.properties.items() -%}\n",
            "                {{- prop_name -}}\n",
            "                {%- if prop_name not in (param_spec.required or []) -%}\n",
            "                    {{- \"?\" }}\n",
            "                {%- endif -%}\n",
            "                {{- \": \" }}\n",
            "                {{ render_typescript_type(prop_spec, param_spec.required or []) }}\n",
            "                {%- if not loop.last -%}\n",
            "                    {{-\", \" }}\n",
            "                {%- endif -%}\n",
            "            {%- endfor -%}\n",
            "            {{- \"}\" }}\n",
            "        {%- else -%}\n",
            "            {{- \"object\" }}\n",
            "        {%- endif -%}\n",
            "    {%- else -%}\n",
            "        {{- \"any\" }}\n",
            "    {%- endif -%}\n",
            "{%- endmacro -%}\n",
            "\n",
            "{%- macro render_tool_namespace(namespace_name, tools) -%}\n",
            "    {{- \"## \" + namespace_name + \"\\n\\n\" }}\n",
            "    {{- \"namespace \" + namespace_name + \" {\\n\\n\" }}\n",
            "    {%- for tool in tools %}\n",
            "        {%- set tool = tool.function %}\n",
            "        {{- \"// \" + tool.description + \"\\n\" }}\n",
            "        {{- \"type \"+ tool.name + \" = \" }}\n",
            "        {%- if tool.parameters and tool.parameters.properties -%}\n",
            "            {{- \"(_: \" }}\n",
            "            {{- \"{\\n\" }}\n",
            "            {%- for param_name, param_spec in tool.parameters.properties.items() %}\n",
            "                {{- \"// \" + param_spec.description + \"\\n\" }}\n",
            "                {{- param_name }}\n",
            "                {%- if param_name not in (tool.parameters.required or []) -%}\n",
            "                    {{- \"?\" }}\n",
            "                {%- endif -%}\n",
            "                {{- \": \" }}\n",
            "                {{- render_typescript_type(param_spec, tool.parameters.required or []) }}\n",
            "                {%- if param_spec.default is defined -%}\n",
            "                    {%- if param_spec.enum %}\n",
            "                        {{- \", // default: \" + param_spec.default }}\n",
            "                    {%- elif param_spec.oneOf %}\n",
            "                        {{- \"// default: \" + param_spec.default }}\n",
            "                    {%- else %}\n",
            "                        {{- \", // default: \" + param_spec.default|tojson }}\n",
            "                    {%- endif -%}\n",
            "                {%- endif -%}\n",
            "                {%- if not loop.last %}\n",
            "                    {{- \",\\n\" }}\n",
            "                {%- else %}\n",
            "                    {{- \"\\n\" }}\n",
            "                {%- endif -%}\n",
            "            {%- endfor %}\n",
            "            {{- \"}) => any;\\n\\n\" }}\n",
            "        {%- else -%}\n",
            "            {{- \"() => any;\\n\\n\" }}\n",
            "        {%- endif -%}\n",
            "    {%- endfor %}\n",
            "    {{- \"} // namespace \" + namespace_name }}\n",
            "{%- endmacro -%}\n",
            "\n",
            "{%- macro render_builtin_tools(browser_tool, python_tool) -%}\n",
            "    {%- if browser_tool %}\n",
            "        {{- \"## browser\\n\\n\" }}\n",
            "        {{- \"// Tool for browsing.\\n\" }}\n",
            "        {{- \"// The `cursor` appears in brackets before each browsing display: `[{cursor}]`.\\n\" }}\n",
            "        {{- \"// Cite information from the tool using the following format:\\n\" }}\n",
            "        {{- \"// `„Äê{cursor}‚Ä†L{line_start}(-L{line_end})?„Äë`, for example: `„Äê6‚Ä†L9-L11„Äë` or `„Äê8‚Ä†L3„Äë`.\\n\" }}\n",
            "        {{- \"// Do not quote more than 10 words directly from the tool output.\\n\" }}\n",
            "        {{- \"// sources=web (default: web)\\n\" }}\n",
            "        {{- \"namespace browser {\\n\\n\" }}\n",
            "        {{- \"// Searches for information related to `query` and displays `topn` results.\\n\" }}\n",
            "        {{- \"type search = (_: {\\n\" }}\n",
            "        {{- \"query: string,\\n\" }}\n",
            "        {{- \"topn?: number, // default: 10\\n\" }}\n",
            "        {{- \"source?: string,\\n\" }}\n",
            "        {{- \"}) => any;\\n\\n\" }}\n",
            "        {{- \"// Opens the link `id` from the page indicated by `cursor` starting at line number `loc`, showing `num_lines` lines.\\n\" }}\n",
            "        {{- \"// Valid link ids are displayed with the formatting: `„Äê{id}‚Ä†.*„Äë`.\\n\" }}\n",
            "        {{- \"// If `cursor` is not provided, the most recent page is implied.\\n\" }}\n",
            "        {{- \"// If `id` is a string, it is treated as a fully qualified URL associated with `source`.\\n\" }}\n",
            "        {{- \"// If `loc` is not provided, the viewport will be positioned at the beginning of the document or centered on the most relevant passage, if available.\\n\" }}\n",
            "        {{- \"// Use this function without `id` to scroll to a new location of an opened page.\\n\" }}\n",
            "        {{- \"type open = (_: {\\n\" }}\n",
            "        {{- \"id?: number | string, // default: -1\\n\" }}\n",
            "        {{- \"cursor?: number, // default: -1\\n\" }}\n",
            "        {{- \"loc?: number, // default: -1\\n\" }}\n",
            "        {{- \"num_lines?: number, // default: -1\\n\" }}\n",
            "        {{- \"view_source?: boolean, // default: false\\n\" }}\n",
            "        {{- \"source?: string,\\n\" }}\n",
            "        {{- \"}) => any;\\n\\n\" }}\n",
            "        {{- \"// Finds exact matches of `pattern` in the current page, or the page given by `cursor`.\\n\" }}\n",
            "        {{- \"type find = (_: {\\n\" }}\n",
            "        {{- \"pattern: string,\\n\" }}\n",
            "        {{- \"cursor?: number, // default: -1\\n\" }}\n",
            "        {{- \"}) => any;\\n\\n\" }}\n",
            "        {{- \"} // namespace browser\\n\\n\" }}\n",
            "    {%- endif -%}\n",
            "\n",
            "    {%- if python_tool %}\n",
            "        {{- \"## python\\n\\n\" }}\n",
            "        {{- \"Use this tool to execute Python code in your chain of thought. The code will not be shown to the user. This tool should be used for internal reasoning, but not for code that is intended to be visible to the user (e.g. when creating plots, tables, or files).\\n\\n\" }}\n",
            "        {{- \"When you send a message containing Python code to python, it will be executed in a stateful Jupyter notebook environment. python will respond with the output of the execution or time out after 120.0 seconds. The drive at '/mnt/data' can be used to save and persist user files. Internet access for this session is UNKNOWN. Depends on the cluster.\\n\\n\" }}\n",
            "    {%- endif -%}\n",
            "{%- endmacro -%}\n",
            "\n",
            "{#- System Message Construction ============================================ #}\n",
            "{%- macro build_system_message() -%}\n",
            "    {%- if model_identity is not defined %}\n",
            "        {{- \"You are ChatGPT, a large language model trained by OpenAI.\\n\" -}}\n",
            "    {%- else %}\n",
            "        {{- model_identity }}\n",
            "    {%- endif %}\n",
            "    {{- \"Knowledge cutoff: 2024-06\\n\" }}\n",
            "    {{- \"Current date: \" + strftime_now(\"%Y-%m-%d\") + \"\\n\\n\" }}\n",
            "    {%- if reasoning_effort is not defined %}\n",
            "        {%- set reasoning_effort = \"medium\" %}\n",
            "    {%- endif %}\n",
            "    {{- \"Reasoning: \" + reasoning_effort + \"\\n\\n\" }}\n",
            "    {%- if builtin_tools is defined %}\n",
            "        {{- \"# Tools\\n\\n\" }}\n",
            "        {%- set available_builtin_tools = namespace(browser=false, python=false) %}\n",
            "        {%- for tool in builtin_tools %}\n",
            "            {%- if tool == \"browser\" %}\n",
            "                {%- set available_builtin_tools.browser = true %}\n",
            "            {%- elif tool == \"python\" %}\n",
            "                {%- set available_builtin_tools.python = true %}\n",
            "            {%- endif %}\n",
            "        {%- endfor %}\n",
            "        {{- render_builtin_tools(available_builtin_tools.browser, available_builtin_tools.python) }}\n",
            "    {%- endif -%}\n",
            "    {{- \"# Valid channels: analysis, commentary, final. Channel must be included for every message.\" }}\n",
            "    {%- if tools is defined -%}\n",
            "        {{- \"\\nCalls to these tools must go to the commentary channel: 'functions'.\" }}\n",
            "    {%- endif -%}\n",
            "{%- endmacro -%}\n",
            "\n",
            "{#- Main Template Logic ================================================= #}\n",
            "{#- Set defaults #}\n",
            "\n",
            "{#- Render system message #}\n",
            "{{- \"<|start|>system<|message|>\" }}\n",
            "{{- build_system_message() }}\n",
            "{{- \"<|end|>\" }}\n",
            "\n",
            "{#- Extract developer message #}\n",
            "{%- if messages[0].role == \"developer\" or messages[0].role == \"system\" %}\n",
            "    {%- set developer_message = messages[0].content %}\n",
            "    {%- set loop_messages = messages[1:] %}\n",
            "{%- else %}\n",
            "    {%- set developer_message = \"\" %}\n",
            "    {%- set loop_messages = messages %}\n",
            "{%- endif %}\n",
            "\n",
            "{#- Render developer message #}\n",
            "{%- if developer_message or tools %}\n",
            "    {{- \"<|start|>developer<|message|>\" }}\n",
            "    {%- if developer_message %}\n",
            "        {{- \"# Instructions\\n\\n\" }}\n",
            "        {{- developer_message }}\n",
            "    {%- endif %}\n",
            "    {%- if tools -%}\n",
            "        {{- \"\\n\\n\" }}\n",
            "        {{- \"# Tools\\n\\n\" }}\n",
            "        {{- render_tool_namespace(\"functions\", tools) }}\n",
            "    {%- endif -%}\n",
            "    {{- \"<|end|>\" }}\n",
            "{%- endif %}\n",
            "\n",
            "{#- Render messages #}\n",
            "{%- set last_tool_call = namespace(name=none) %}\n",
            "{%- for message in loop_messages -%}\n",
            "    {#- At this point only assistant/user/tool messages should remain #}\n",
            "    {%- if message.role == 'assistant' -%}\n",
            "        {%- if \"tool_calls\" in message %}\n",
            "            {#- We assume max 1 tool call per message, and so we infer the tool call name #}\n",
            "            {#- in \"tool\" messages from the most recent assistant tool call name #}\n",
            "            {%- set tool_call = message.tool_calls[0] %}\n",
            "            {%- if tool_call.function %}\n",
            "                {%- set tool_call = tool_call.function %}\n",
            "            {%- endif %}\n",
            "            {%- if message.content %}\n",
            "                {{- \"<|start|>assistant<|channel|>analysis<|message|>\" + message.content + \"<|end|>\" }}\n",
            "            {%- endif %}\n",
            "            {{- \"<|start|>assistant to=\" }}\n",
            "            {{- \"functions.\" + tool_call.name + \"<|channel|>commentary json<|message|>\" }}\n",
            "            {{- tool_call.arguments|tojson }}\n",
            "            {{- \"<|call|>\" }}\n",
            "            {%- set last_tool_call.name = tool_call.name %}\n",
            "        {%- elif \"thinking\" in message and loop.last and not add_generation_prompt %}\n",
            "            {#- Only render the CoT if the final turn is an assistant turn and add_generation_prompt is false #}\n",
            "            {#- This is a situation that should only occur in training, never in inference. #}\n",
            "            {{- \"<|start|>assistant<|channel|>analysis<|message|>\" + message.thinking + \"<|end|>\" }}\n",
            "            {#- <|return|> indicates the end of generation, but <|end|> does not #}\n",
            "            {#- <|return|> should never be an input to the model, but we include it as the final token #}\n",
            "            {#- when training, so the model learns to emit it. #}\n",
            "            {{- \"<|start|>assistant<|channel|>final<|message|>\" + message.content + \"<|return|>\" }}\n",
            "            {%- set last_tool_call.name = none %}\n",
            "        {%- elif \"thinking\" in message %}\n",
            "            {#- CoT is dropped during all previous turns, so we never render it for inference #}\n",
            "            {{- \"<|start|>assistant<|channel|>final<|message|>\" + message.content + \"<|end|>\" }}\n",
            "            {%- set last_tool_call.name = none %}\n",
            "        {%- elif loop.last and not add_generation_prompt %}\n",
            "            {#- <|return|> indicates the end of generation, but <|end|> does not #}\n",
            "            {#- <|return|> should never be an input to the model, but we include it as the final token #}\n",
            "            {#- when training, so the model learns to emit it. #}\n",
            "            {{- \"<|start|>assistant<|message|>\" + message.content + \"<|return|>\" }}\n",
            "        {%- else %}\n",
            "            {{- \"<|start|>assistant<|message|>\" + message.content + \"<|end|>\" }}\n",
            "            {%- set last_tool_call.name = none %}\n",
            "        {%- endif %}\n",
            "    {%- elif message.role == 'tool' -%}\n",
            "        {%- if last_tool_call.name is none %}\n",
            "            {{- raise_exception(\"Message has tool role, but there was no previous assistant message with a tool call!\") }}\n",
            "        {%- endif %}\n",
            "        {{- \"<|start|>functions.\" + last_tool_call.name }}\n",
            "        {{- \" to=assistant<|channel|>commentary<|message|>\" + message.content|tojson + \"<|end|>\" }}\n",
            "    {%- else -%}\n",
            "        {{- \"<|start|>user<|message|>\" + message.content + \"<|end|>\" }}\n",
            "    {%- endif -%}\n",
            "{%- endfor -%}\n",
            "\n",
            "{#- Generation prompt #}\n",
            "{%- if add_generation_prompt -%}\n",
            "<|start|>assistant\n",
            "{%- endif -%}\n",
            "{# Copyright 2025-present Unsloth. Apache 2.0 License. Unsloth chat template fixes. Edited from ggml-org & OpenAI #}\n",
            "Using chat eos_token: <|return|>\n",
            "Using chat bos_token: <|startoftext|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.create_chat_completion(\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What is the capital of France?\"\n",
        "        }\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "WKgwzeBdmKXt",
        "outputId": "60af2130-72bf-4207-e76e-198538ce96c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   49367.86 ms\n",
            "llama_perf_context_print: prompt eval time =   49367.61 ms /    88 tokens (  561.00 ms per token,     1.78 tokens per second)\n",
            "llama_perf_context_print:        eval time =   60777.69 ms /    42 runs   ( 1447.09 ms per token,     0.69 tokens per second)\n",
            "llama_perf_context_print:       total time =  110232.39 ms /   130 tokens\n",
            "llama_perf_context_print:    graphs reused =         39\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'chatcmpl-67b1fd8b-f18f-4066-801f-77c5435547fa',\n",
              " 'object': 'chat.completion',\n",
              " 'created': 1769939642,\n",
              " 'model': '/root/.cache/huggingface/hub/models--DavidAU--OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/snapshots/92d1bcc0244193a2cd11b9da138dfee9e5203d37/./OpenAI-20B-NEO-CODE-DI-Uncensored-Q5_1.gguf',\n",
              " 'choices': [{'index': 0,\n",
              "   'message': {'role': 'assistant',\n",
              "    'content': '<|channel|>analysis<|message|>The user asks: \"What is the capital of France?\" The answer: Paris. Provide concise answer.<|end|><|start|>assistant<|channel|>commentary to=functions <|constrain|>json<|message|>{\"response\":\"Paris\"}'},\n",
              "   'logprobs': None,\n",
              "   'finish_reason': 'stop'}],\n",
              " 'usage': {'prompt_tokens': 88, 'completion_tokens': 42, 'total_tokens': 130}}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Xg_oXpSVonnM"
      }
    }
  ]
}